{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is a modeling phase for the FakeNews project\n",
    "\n",
    "Initial classifier has shown a good result with ExtraTreesClassifier (done by pycaret\n",
    "LGBM seems to be somewhat better though\n",
    "Here we will be training and tuning this model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kill the warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main Dependencies:\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob, Word\n",
    "import config\n",
    "\n",
    "# Statistics imports:\n",
    "from statistics import mean\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# some necessary actions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Emotions = ['Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust']\n",
    "wordsData = pd.read_excel(config.wordsData_url, index_col=0)\n",
    "wordsData = wordsData[wordsData.columns.intersection(['English Word']+[emotion for emotion in Emotions])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--- Feature Constructors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_wordsCount(df_row, Sentence, df):\n",
    "    # count the unique words in the Sentence and calculate the ratio\n",
    "    uniqueWords = len(set(Sentence.words))\n",
    "    totalWords = len((Sentence.words))\n",
    "    df.at[df_row,'uniqe_words_ratio']=uniqueWords/totalWords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_nounPolarity(df_row, Sentence, df):\n",
    "    # Add feature for sum of polarity index into the dataset\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    try:\n",
    "        df.at[df_row,'nounPolarity'] = mean([TextBlob(nounS).sentiment.polarity for nounS in Sentence.noun_phrases])\n",
    "    except:\n",
    "        df.at[df_row,'nounPolarity'] = 0 # No nouns found"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_nounSubjectivity(df_row, Sentence, df):\n",
    "    # Add feature for sum of subjectivity index into the dataset\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    try:\n",
    "        df.at[df_row,'nounSubjectivity'] = mean([TextBlob(nounS).sentiment.subjectivity for nounS in Sentence.noun_phrases])\n",
    "    except:\n",
    "        df.at[df_row,'nounSubjectivity'] = 0 # No nouns found"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_sentenceSentiment(df_row, Sentence, df):\n",
    "    # Entire phrase sentiment analysis\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    polarity, subjectivity = Sentence.sentiment\n",
    "    df.at[df_row,'sentencePolarity'] = polarity\n",
    "    df.at[df_row,'sentenceSubjectivity'] = subjectivity\n",
    "    df.at[df_row,'meanPolarity_per_sentence'] = mean([sentence.polarity for sentence in Sentence.sentences])\n",
    "    df.at[df_row,'meanSubjetivity_per_sentence'] = mean([sentence.subjectivity for sentence in Sentence.sentences])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_Emotions(df_row, Sentence, df):\n",
    "    # Insert the emotional count per words into dataset\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    # WordsData is the English dataset, one-hot-encoded for emotions\n",
    "\n",
    "    # Reset emotions for the selected row\n",
    "    for emotion in Emotions:\n",
    "        df.at[df_row,emotion]=0\n",
    "\n",
    "    for word in [Word(word).singularize().lemmatize() for word in Sentence.words if word in wordsData.index]:\n",
    "        try:\n",
    "            for emotion in set(wordsData.columns[(wordsData[wordsData.index == word].values == 1)[0]].tolist()):\n",
    "                df.at[df_row,emotion]+=1\n",
    "        except:\n",
    "            pass # no emotonal load for that specific word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def frequency_Analysis(df_row, Sentence, df):\n",
    "    # Emotional load converting to frequency and amplitude\n",
    "    # df_row is an index of the row in the dataframe\n",
    "\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    data1 = np.array([sentence.polarity for sentence in Sentence.sentences]) # Sentence polarity\n",
    "    data2 = np.array([sentence.subjectivity for sentence in Sentence.sentences]) # Sentence subjectivity\n",
    "    sentence_timing = [len(sentence.words) for sentence in Sentence.sentences] # Sentence timing\n",
    "\n",
    "    #Frequency Analysis:\n",
    "    ps1 = np.abs(np.fft.fft(data1))**2\n",
    "    ps2 = np.abs(np.fft.fft(data2))**2\n",
    "\n",
    "    time_step = 1 / np.average(sentence_timing)\n",
    "    freqs1 = np.fft.fftfreq(data1.size, time_step)\n",
    "    freqs2 = np.fft.fftfreq(data2.size, time_step)\n",
    "\n",
    "    MaxPolarityFrequency = round(max(freqs1),2) # Feature\n",
    "    MaxSubjectivityFrequency = round(max(freqs2),2) # Feature\n",
    "\n",
    "    df.at[df_row,'MaxPolarityFrequency'] = MaxPolarityFrequency\n",
    "    df.at[df_row,'MaxSubjectivityFrequency'] = MaxSubjectivityFrequency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def correlation_and_entropy(df_row,Sentence,df):\n",
    "    # Test for mutual correlation of sentences polarity and subjectivity\n",
    "    # df_row is an index of the row in the dataframe\n",
    "\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    data1 = np.array([sentence.polarity for sentence in Sentence.sentences]) # Sentence polarity\n",
    "    data2 = np.array([sentence.subjectivity for sentence in Sentence.sentences]) # Sentence subjectivity\n",
    "\n",
    "    # Peason correlation between polarity and subjectivity - Feature\n",
    "    try:\n",
    "        corrP, _ = pearsonr(data1, data2)\n",
    "    except:\n",
    "        corrP = 0 # less than 2 elements for correlation\n",
    "    # Spearman correlation between polarity and subjectivity - Feature\n",
    "    try:\n",
    "        corrS, _ = spearmanr(data1, data2)\n",
    "    except:\n",
    "        corrS = 0 # less than 2 elements for correlation\n",
    "\n",
    "    # Calculate entropy of words in the sentence\n",
    "    p_data = pd.DataFrame(Sentence.words).value_counts()\n",
    "    try:\n",
    "        entropy = scipy.stats.entropy(p_data)\n",
    "    except:\n",
    "        entropy = 0 # No data for entropy calculation\n",
    "\n",
    "    df.at[df_row,'corrP'] = corrP\n",
    "    df.at[df_row,'corrS'] = corrS\n",
    "    df.at[df_row,'entropy'] = entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def construct_Features(indexRange,df,correct=True):\n",
    "    # Construct the features\n",
    "    for row in indexRange:\n",
    "        print(f'Constructing features for row #{row} out of {len(df)}:')\n",
    "        try:\n",
    "            if correct:\n",
    "                Sentence = TextBlob(df['text'][row]).correct()\n",
    "            else:\n",
    "                Sentence = TextBlob(df['text'][row])\n",
    "\n",
    "            feature_wordsCount(row,Sentence,df)\n",
    "            feature_nounPolarity(row, Sentence,df)\n",
    "            feature_nounSubjectivity(row, Sentence,df)\n",
    "            feature_sentenceSentiment(row, Sentence,df)\n",
    "            feature_Emotions(row, Sentence, df)\n",
    "            frequency_Analysis(row, Sentence, df)\n",
    "            correlation_and_entropy(row, Sentence, df)\n",
    "        except:\n",
    "            print(f'row #{row} contains some bugs, skipping')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the df with features\n",
    "df = pd.read_csv('Data/fake_news_features_non_corrected.csv').drop(['Unnamed: 0'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Let's leave just the features and target values\n",
    "df = df.drop(['text'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df.drop(['class'],axis=1)\n",
    "y = df['class']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, boosting_type='gbdt',\n",
    "                       class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,\n",
    "                       importance_type='split', learning_rate=0.3, max_depth=-1,\n",
    "                       min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,\n",
    "                       n_estimators=190, n_jobs=-1, num_leaves=20, objective=None,\n",
    "                       random_state=786, reg_alpha=1e-06, reg_lambda=5, silent='warn',\n",
    "                       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "model.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TrueText = \"Snapchat could become more popular with advertisers than Twitter  Yahoo and AOL within three years  with the messaging app company forecast to be bring in revenues of more than $3bn (Â£2.4bn) a year by the end of 2019. That bullish forecast is based on advertisers targeting the hard-to-reach youth audience that Snapchat has seemingly cornered. More than half (51%) of video users on the app are under 24  compared with 23% for Facebook and 17% for Google's YouTube (17%)  according to Ampere Analysis. Brands are also keen to see a true rival emerge to challenge Facebook and Google  which have recently come in for heavy criticism for their advertising practices. The two web giants currently account for 58% of the $141bn global mobile ad market.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FakeText = \"Yahoo and AOL could be extremely popular over the next decade and revenues are expected to skyrocket by 2020.  This forecast is based on the advertisers that target a younger audience.  Half of the users are under the age of 30 compared to facebook and google which cover the older market, as per the recent analysis posting by the Washington Post.  Facebook and google will be challenged.  The current advertising practices have received extreme criticism, the web giants currently hold a 50% stake in the global ad market and are currently seeing a small decline in their users.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testDF = pd.DataFrame(columns=['text', 'uniqe_words_ratio', 'nounPolarity', 'nounSubjectivity',\n",
    "                               'sentencePolarity', 'sentenceSubjectivity', 'meanPolarity_per_sentence',\n",
    "                               'meanSubjetivity_per_sentence', 'Anger', 'Anticipation', 'Disgust',\n",
    "                               'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust', 'MaxPolarityFrequency',\n",
    "                               'MaxSubjectivityFrequency', 'corrP', 'corrS', 'entropy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testDF.at[0,'text'] = TrueText\n",
    "construct_Features(range(1),testDF,correct=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.predict(testDF.drop(['text'],axis=1).astype(float))[0] # Shall be True (1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testDF.at[0,'text'] = FakeText\n",
    "construct_Features(range(1),testDF,correct=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.predict(testDF.drop(['text'],axis=1).astype(float))[0] # Shall be False (0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save\n",
    "import joblib\n",
    "joblib.dump(model, \"model.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load\n",
    "import joblib\n",
    "model = joblib.load(\"model.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}