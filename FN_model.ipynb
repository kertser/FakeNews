{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is a modeling phase for the FakeNews project\n",
    "\n",
    "Initial classifier has shown a good result with ExtraTreesClassifier (done by pycaret\n",
    "LGBM seems to be somewhat better though\n",
    "Here we will be training and tuning this model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimized Parameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
    "                     criterion='gini', max_depth=10, max_features='sqrt',\n",
    "                     max_leaf_nodes=None, max_samples=None,\n",
    "                     min_impurity_decrease=0, min_impurity_split=None,\n",
    "                     min_samples_leaf=6, min_samples_split=7,\n",
    "                     min_weight_fraction_leaf=0.0, n_estimators=190, n_jobs=-1,\n",
    "                     oob_score=False, random_state=786, verbose=0,\n",
    "                     warm_start=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=10, max_features='sqrt',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0, min_impurity_split=None,\n",
    "                       min_samples_leaf=6, min_samples_split=7,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=190,\n",
    "                       n_jobs=-1, oob_score=False, random_state=786, verbose=0,\n",
    "                       warm_start=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, boosting_type='gbdt',\n",
    "               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,\n",
    "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
    "               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,\n",
    "               n_estimators=190, n_jobs=-1, num_leaves=20, objective=None,\n",
    "               random_state=786, reg_alpha=1e-06, reg_lambda=5, silent='warn',\n",
    "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Kill the warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Main Dependencies:\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob, Word\n",
    "import config\n",
    "\n",
    "# Statistics imports:\n",
    "from statistics import mean\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# some necessary actions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Emotions = ['Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust']\n",
    "wordsData = pd.read_excel(config.wordsData_url, index_col=0)\n",
    "wordsData = wordsData[wordsData.columns.intersection(['English Word']+[emotion for emotion in Emotions])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--- Feature Constructors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def feature_wordsCount(df_row, Sentence, df):\n",
    "    # count the unique words in the Sentence and calculate the ratio\n",
    "    uniqueWords = len(set(Sentence.words))\n",
    "    totalWords = len((Sentence.words))\n",
    "    df.at[df_row,'uniqe_words_ratio']=uniqueWords/totalWords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def feature_nounPolarity(df_row, Sentence, df):\n",
    "    # Add feature for sum of polarity index into the dataset\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    try:\n",
    "        df.at[df_row,'nounPolarity'] = mean([TextBlob(nounS).sentiment.polarity for nounS in Sentence.noun_phrases])\n",
    "    except:\n",
    "        df.at[df_row,'nounPolarity'] = 0 # No nouns found"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def feature_nounSubjectivity(df_row, Sentence, df):\n",
    "    # Add feature for sum of subjectivity index into the dataset\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    try:\n",
    "        df.at[df_row,'nounSubjectivity'] = mean([TextBlob(nounS).sentiment.subjectivity for nounS in Sentence.noun_phrases])\n",
    "    except:\n",
    "        df.at[df_row,'nounSubjectivity'] = 0 # No nouns found"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def feature_sentenceSentiment(df_row, Sentence, df):\n",
    "    # Entire phrase sentiment analysis\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    polarity, subjectivity = Sentence.sentiment\n",
    "    df.at[df_row,'sentencePolarity'] = polarity\n",
    "    df.at[df_row,'sentenceSubjectivity'] = subjectivity\n",
    "    df.at[df_row,'meanPolarity_per_sentence'] = mean([sentence.polarity for sentence in Sentence.sentences])\n",
    "    df.at[df_row,'meanSubjetivity_per_sentence'] = mean([sentence.subjectivity for sentence in Sentence.sentences])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def feature_Emotions(df_row, Sentence, df):\n",
    "    # Insert the emotional count per words into dataset\n",
    "    # df_row is an index of the row in the dataframe\n",
    "    # WordsData is the English dataset, one-hot-encoded for emotions\n",
    "\n",
    "    # Reset emotions for the selected row\n",
    "    for emotion in Emotions:\n",
    "        df.at[df_row,emotion]=0\n",
    "\n",
    "    for word in [Word(word).singularize().lemmatize() for word in Sentence.words if word in wordsData.index]:\n",
    "        try:\n",
    "            for emotion in set(wordsData.columns[(wordsData[wordsData.index == word].values == 1)[0]].tolist()):\n",
    "                df.at[df_row,emotion]+=1\n",
    "        except:\n",
    "            pass # no emotonal load for that specific word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def frequency_Analysis(df_row, Sentence, df):\n",
    "    # Emotional load converting to frequency and amplitude\n",
    "    # df_row is an index of the row in the dataframe\n",
    "\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    data1 = np.array([sentence.polarity for sentence in Sentence.sentences]) # Sentence polarity\n",
    "    data2 = np.array([sentence.subjectivity for sentence in Sentence.sentences]) # Sentence subjectivity\n",
    "    sentence_timing = [len(sentence.words) for sentence in Sentence.sentences] # Sentence timing\n",
    "\n",
    "    #Frequency Analysis:\n",
    "    ps1 = np.abs(np.fft.fft(data1))**2\n",
    "    ps2 = np.abs(np.fft.fft(data2))**2\n",
    "\n",
    "    time_step = 1 / np.average(sentence_timing)\n",
    "    freqs1 = np.fft.fftfreq(data1.size, time_step)\n",
    "    freqs2 = np.fft.fftfreq(data2.size, time_step)\n",
    "\n",
    "    MaxPolarityFrequency = round(max(freqs1),2) # Feature\n",
    "    MaxSubjectivityFrequency = round(max(freqs2),2) # Feature\n",
    "\n",
    "    df.at[df_row,'MaxPolarityFrequency'] = MaxPolarityFrequency\n",
    "    df.at[df_row,'MaxSubjectivityFrequency'] = MaxSubjectivityFrequency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def correlation_and_entropy(df_row,Sentence,df):\n",
    "    # Test for mutual correlation of sentences polarity and subjectivity\n",
    "    # df_row is an index of the row in the dataframe\n",
    "\n",
    "    #Sentence = TextBlob(fake_news_full_df['text'][df_row]).correct()\n",
    "    data1 = np.array([sentence.polarity for sentence in Sentence.sentences]) # Sentence polarity\n",
    "    data2 = np.array([sentence.subjectivity for sentence in Sentence.sentences]) # Sentence subjectivity\n",
    "\n",
    "    # Peason correlation between polarity and subjectivity - Feature\n",
    "    try:\n",
    "        corrP, _ = pearsonr(data1, data2)\n",
    "    except:\n",
    "        corrP = 0 # less than 2 elements for correlation\n",
    "    # Spearman correlation between polarity and subjectivity - Feature\n",
    "    try:\n",
    "        corrS, _ = spearmanr(data1, data2)\n",
    "    except:\n",
    "        corrS = 0 # less than 2 elements for correlation\n",
    "\n",
    "    # Calculate entropy of words in the sentence\n",
    "    p_data = pd.DataFrame(Sentence.words).value_counts()\n",
    "    try:\n",
    "        entropy = scipy.stats.entropy(p_data)\n",
    "    except:\n",
    "        entropy = 0 # No data for entropy calculation\n",
    "\n",
    "    df.at[df_row,'corrP'] = corrP\n",
    "    df.at[df_row,'corrS'] = corrS\n",
    "    df.at[df_row,'entropy'] = entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def construct_Features(indexRange,df,correct=True):\n",
    "    # Construct the features\n",
    "    for row in indexRange:\n",
    "        print(f'Constructing features for row #{row} out of {len(df)}:')\n",
    "        try:\n",
    "            if correct:\n",
    "                Sentence = TextBlob(df['text'][row]).correct()\n",
    "            else:\n",
    "                Sentence = TextBlob(df['text'][row])\n",
    "\n",
    "            feature_wordsCount(row,Sentence,df)\n",
    "            feature_nounPolarity(row, Sentence,df)\n",
    "            feature_nounSubjectivity(row, Sentence,df)\n",
    "            feature_sentenceSentiment(row, Sentence,df)\n",
    "            feature_Emotions(row, Sentence, df)\n",
    "            frequency_Analysis(row, Sentence, df)\n",
    "            correlation_and_entropy(row, Sentence, df)\n",
    "        except:\n",
    "            print(f'row #{row} contains some bugs, skipping')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Read the df with features\n",
    "df = pd.read_csv('Data/fake_news_features_corrected.csv').drop(['Unnamed: 0'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Let's leave just the features and target values\n",
    "df = df.drop(['text'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "X = df.drop(['class'],axis=1)\n",
    "y = df['class']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, boosting_type='gbdt',\n",
    "               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,\n",
    "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
    "               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,\n",
    "               n_estimators=190, n_jobs=-1, num_leaves=20, objective=None,\n",
    "               random_state=786, reg_alpha=1e-06, reg_lambda=5, silent='warn',\n",
    "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "data": {
      "text/plain": "LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, feature_fraction=0.4,\n               learning_rate=0.3, min_child_samples=61, min_split_gain=0.1,\n               n_estimators=190, num_leaves=20, random_state=786,\n               reg_alpha=1e-06, reg_lambda=5)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, feature_fraction=0.4,\n               learning_rate=0.3, min_child_samples=61, min_split_gain=0.1,\n               n_estimators=190, num_leaves=20, random_state=786,\n               reg_alpha=1e-06, reg_lambda=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, feature_fraction=0.4,\n               learning_rate=0.3, min_child_samples=61, min_split_gain=0.1,\n               n_estimators=190, num_leaves=20, random_state=786,\n               reg_alpha=1e-06, reg_lambda=5)</pre></div></div></div></div></div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMClassifier(bagging_fraction=0.6, bagging_freq=1, boosting_type='gbdt',\n",
    "                       class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,\n",
    "                       importance_type='split', learning_rate=0.3, max_depth=-1,\n",
    "                       min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,\n",
    "                       n_estimators=190, n_jobs=-1, num_leaves=20, objective=None,\n",
    "                       random_state=786, reg_alpha=1e-06, reg_lambda=5, silent='warn',\n",
    "                       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "model.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8119486320491346"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X,y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "TrueText = \"Snapchat could become more popular with advertisers than Twitter  Yahoo and AOL within three years  with the messaging app company forecast to be bring in revenues of more than $3bn (£2.4bn) a year by the end of 2019. That bullish forecast is based on advertisers targeting the hard-to-reach youth audience that Snapchat has seemingly cornered. More than half (51%) of video users on the app are under 24  compared with 23% for Facebook and 17% for Google's YouTube (17%)  according to Ampere Analysis. Brands are also keen to see a true rival emerge to challenge Facebook and Google  which have recently come in for heavy criticism for their advertising practices. The two web giants currently account for 58% of the $141bn global mobile ad market.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "FakeText = \"Yahoo and AOL could be extremely popular over the next decade and revenues are expected to skyrocket by 2020.  This forecast is based on the advertisers that target a younger audience.  Half of the users are under the age of 30 compared to facebook and google which cover the older market, as per the recent analysis posting by the Washington Post.  Facebook and google will be challenged.  The current advertising practices have received extreme criticism, the web giants currently hold a 50% stake in the global ad market and are currently seeing a small decline in their users.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "testDF = pd.DataFrame(columns=['text', 'uniqe_words_ratio', 'nounPolarity', 'nounSubjectivity',\n",
    "                               'sentencePolarity', 'sentenceSubjectivity', 'meanPolarity_per_sentence',\n",
    "                               'meanSubjetivity_per_sentence', 'Anger', 'Anticipation', 'Disgust',\n",
    "                               'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust', 'MaxPolarityFrequency',\n",
    "                               'MaxSubjectivityFrequency', 'corrP', 'corrS', 'entropy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing features for row #0 out of 1:\n"
     ]
    }
   ],
   "source": [
    "testDF.at[0,'text'] = TrueText\n",
    "construct_Features(range(1),testDF,correct=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text uniqe_words_ratio  \\\n0  Snapchat could become more popular with advert...          0.738095   \n\n  nounPolarity nounSubjectivity sentencePolarity sentenceSubjectivity  \\\n0    -0.041176         0.058824         0.143939             0.442424   \n\n  meanPolarity_per_sentence meanSubjetivity_per_sentence Anger Anticipation  \\\n0                  0.105556                     0.337778     3            5   \n\n   ... Fear Joy Sadness Surprise Trust MaxPolarityFrequency  \\\n0  ...    2   1       1        1     3                10.08   \n\n  MaxSubjectivityFrequency     corrP     corrS   entropy  \n0                    10.08  0.680393  0.666886  4.394875  \n\n[1 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>uniqe_words_ratio</th>\n      <th>nounPolarity</th>\n      <th>nounSubjectivity</th>\n      <th>sentencePolarity</th>\n      <th>sentenceSubjectivity</th>\n      <th>meanPolarity_per_sentence</th>\n      <th>meanSubjetivity_per_sentence</th>\n      <th>Anger</th>\n      <th>Anticipation</th>\n      <th>...</th>\n      <th>Fear</th>\n      <th>Joy</th>\n      <th>Sadness</th>\n      <th>Surprise</th>\n      <th>Trust</th>\n      <th>MaxPolarityFrequency</th>\n      <th>MaxSubjectivityFrequency</th>\n      <th>corrP</th>\n      <th>corrS</th>\n      <th>entropy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Snapchat could become more popular with advert...</td>\n      <td>0.738095</td>\n      <td>-0.041176</td>\n      <td>0.058824</td>\n      <td>0.143939</td>\n      <td>0.442424</td>\n      <td>0.105556</td>\n      <td>0.337778</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>10.08</td>\n      <td>10.08</td>\n      <td>0.680393</td>\n      <td>0.666886</td>\n      <td>4.394875</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testDF.drop(['text'],axis=1).astype(float))[0] # Shall be True (1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing features for row #0 out of 1:\n"
     ]
    }
   ],
   "source": [
    "testDF.at[0,'text'] = FakeText\n",
    "construct_Features(range(1),testDF,correct=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testDF.drop(['text'],axis=1).astype(float))[0] # Shall be False (0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "['model1a.pkl']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "import joblib\n",
    "joblib.dump(model, \"model1a.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load\n",
    "import joblib\n",
    "model = joblib.load(\"model.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}